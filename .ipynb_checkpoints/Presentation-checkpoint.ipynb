{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring and predicting locality of smartphone memory access through data mining of trace files\n",
    "### IITGN CS612 Fall 2017 - Project Checkpoint Presentation (13 November 2017)\n",
    "### Sohhom Bandyopadhyay (15510011) and Sujata Sinha (15350008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Based on trace files from: ** <br>\n",
    "http://iotta.snia.org/tracetypes/3 (Traces collected from **Nexus 5**) <br>\n",
    "Zhou, D., Pan, W., Wang, W., & Xie, T. (2015, October). I/O characteristics of smartphone applications and their implications for eMMC design. In Workload Characterization (IISWC), 2015 IEEE International Symposium on (pp. 12-21). IEEE.\n",
    "\n",
    "\n",
    "\n",
    "** Outline (this notebook):**\n",
    "This notebook presents the code and outputs of some preliminary EDA (Exploratory Data Analysis). Sections:\n",
    " - Description of trace file format\n",
    " - Conversion and data preprocessing\n",
    " - Calculate turnaround time and hardware processing time for each access, based on timestamps\n",
    " - plots of:\n",
    "    - Average request size (bytes) by type of activity\n",
    "    - Average turnaround time (microseconds) by type of activity\n",
    "    - Average hardware time (microseconds) by type of activity\n",
    "    \n",
    "\n",
    "** ToDo (next two weeks): ** <br>\n",
    " - measuring spatial and temporal locality\n",
    " - Machine learning classifier to predict the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace file format:\n",
    "**column 0** : start address (in sectors) <br>\n",
    "**column 1** : access size (in sectors) <br>\n",
    "**column 2** : access size (in byte) <br>\n",
    "**column 3** : access type & waiting status (3 bit number):\n",
    " - LSB: indicates read (0) or write (1)\n",
    " - MSB: indicates waiting status (0 = yes, 1 = no)\n",
    " - Middle bit: unused <br>\n",
    " (It's not represented as binary, but as integers : 0, 1, 4 and 5 )\n",
    " <br>\n",
    " \n",
    "**column 4** : request generate time (generated and inserted into request queue). <br>\n",
    "**column 5** : request process start time (fetched and and began processing)  <br>\n",
    "**column 6** : request submit time (submitted to hardware) <br>\n",
    "**column 7** : request finish time (completed, callback function invoked)  <br>\n",
    "\n",
    "Thus, any request goes through 4 stages: push to queue -> start processing -> submit to hardware -> finish (callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define where the trace files, w.r.t working directory\n",
    "data_dir = 'Trace_files'\n",
    "# collect the names of all txt files in specified directory\n",
    "fnames = [x for x in filter(lambda x: x.endswith('.txt'), os.listdir(os.path.join('.', data_dir)))]\n",
    "# extract the names of workloads from file names, this will be useful later to create dictionary keys\n",
    "workload_names = [ x for x in map(lambda x: x.split('_')[1].split('.')[0], fnames) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read all the Trace Files into a single variable called dataset\n",
    "# it's dict of dataframes, indexed by name\n",
    "# example: dataset['twitter'] gives all rows of the file \"log186_twitter.txt\"\n",
    "dataset = {fname.split('_')[1].split('.')[0]:pd.read_csv(os.path.join(data_dir, fname), delimiter='\\s+', header = None, dtype = float) for fname in fnames}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert unix timestamps to datetime objects\n",
    "This will help with time difference calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# columns 4 through 7 contains the timestamps (floating point numbers), need to convert them into manipulable objects\n",
    "for cindx in 4, 5, 6, 7:\n",
    "    for df in dataset.values():\n",
    "        # apply transformation (non destructive) to each column, then store the updated column back in the dataframe\n",
    "        df.loc[:,cindx] = df.loc[:,cindx].apply(datetime.fromtimestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate durations\n",
    " ... of various processing stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize empty dictionaries, will be indexed by workload names\n",
    "# refer to trace file format for details\n",
    "# e.g turnaround_time['twitter'] gives a vector of same length as trace file, containing the TaT for each request\n",
    "turnaround_time = {}\n",
    "hw_time = {}\n",
    "for name in workload_names:\n",
    "    # TaT = finish time (column 7) - submission time (column 4)\n",
    "    turnaround_time[name] = dataset[name].iloc[:,7] - dataset[name].iloc[:,4]\n",
    "    turnaround_time[name] = turnaround_time[name].apply(lambda x: x.components.milliseconds*1000+x.components.microseconds)\n",
    "    # hardware processing time = finish time (column 7) - hardware submit time (column 6)\n",
    "    hw_time[name] = dataset[name].iloc[:,7] - dataset[name].iloc[:,6]\n",
    "    hw_time[name] = hw_time[name].apply(lambda x: x.components.milliseconds*1000+x.components.microseconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot averages across activities\n",
    "Just as a preliminary exploration, we are interested in finding out the relative magnitudes across workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract the request sizes for each activity\n",
    "# dictionary indexed by workload names, each element a numpy ndarray\n",
    "# e.g request_sizes['twitter'] gives the request size (in bytes) of all requests in that trace file\n",
    "request_sizes = { workname:dataset[workname].iloc[:,2] for workname in workload_names }\n",
    "# calculate the average across all workloads, again a dictionary indexed by workload names\n",
    "avg_req_size = {x:np.mean(request_sizes[x]) for x in workload_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting..\n",
    "somevar = seaborn.barplot(list(avg_req_size.keys()), list(avg_req_size.values()), palette='dark')\n",
    "for item in somevar.get_xticklabels():\n",
    "    item.set_rotation(90)\n",
    "plt.ylabel('Request size (bytes)')\n",
    "plt.xlabel('Workload')\n",
    "plt.title(\"Average request size across workloads\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO be plotted\n",
    "# calculate the average turnaround time (request finish - request submission) across all workloads\n",
    "avg_tat = {x:np.mean(turnaround_time[x]) for x in workload_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting..\n",
    "axes = seaborn.barplot(list(avg_tat.keys()), list(avg_tat.values()), palette='dark')\n",
    "for item in axes.get_xticklabels():\n",
    "    item.set_rotation(85)\n",
    "plt.title(\"Average turnaround time across workloads\")\n",
    "plt.xlabel('Workload')\n",
    "plt.ylabel('Turnaround time (microsecond)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO be plotted\n",
    "# calculate the average time taken by hardware to process request, across all workloads\n",
    "avg_hw = {x:np.mean(hw_time[x]) for x in workload_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting..\n",
    "axes = seaborn.barplot(list(avg_hw.keys()), list(avg_hw.values()), palette='dark')\n",
    "for item in axes.get_xticklabels():\n",
    "    item.set_rotation(85)\n",
    "plt.title(\"Average hardware time across workloads\")\n",
    "plt.xlabel('Workload')\n",
    "plt.ylabel('Hardware time (microsecond)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some interesting questions :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "  - How many unique sector requests?\n",
    "  - Does request size correlate with the time taken to service it?\n",
    "  - How many repeated requests within a certain threshold of time (e.g. 5ms)?\n",
    "  - Assuming a certain cache size and configuration, how many bytes could have been cached?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot 4 (fraction of unique sector requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate and plot number of unique sectors across workloads\n",
    "unique_fraction = {key:dataset[key].iloc[:,0].unique().shape[0] / dataset[key].shape[0] for key in workload_names}\n",
    "# plot them..\n",
    "axes = seaborn.barplot(list(unique_fraction.keys()), list(unique_fraction.values()), palette='dark')\n",
    "for item in axes.get_xticklabels():\n",
    "    item.set_rotation(85)\n",
    "plt.title(\"Fraction of requests that are unique across workloads\")\n",
    "plt.xlabel('Workload')\n",
    "plt.ylabel('Fraction of unique requests')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot 5 (corelation of request size with hardware processing time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlate request size with hardware processing time (i.e, ignore the queue waiting, if any)\n",
    "corrcoefs_reqsize_hwtime = {key:hw_time[key].corr(request_sizes[key]) for key in workload_names}\n",
    "# plotting\n",
    "axes = seaborn.barplot(list(corrcoefs_reqsize_hwtime.keys()), list(corrcoefs_reqsize_hwtime.values()), palette='dark')\n",
    "for item in axes.get_xticklabels():\n",
    "    item.set_rotation(85)\n",
    "plt.title(\"Correlation coefficient between hardware processing time and request size\")\n",
    "plt.xlabel('Workload')\n",
    "plt.ylabel('Correlation coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### towards locality..\n",
    "For a given workload, for each sector that was requested **more than once**, what are the **time-deltas between first request and subsequent requests?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = dataset['radio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_sectors = df.iloc[:,0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_sectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5820-1698"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
